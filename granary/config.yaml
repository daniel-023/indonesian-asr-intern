documentation: |
  Granary Dataset Creation Pipeline
  =================================

  Overview
  --------

  This configuration drives the **Granary pseudo-labelling pipeline** – an
  open-source workflow that transforms large, noisy speech corpora into
  high-quality Automatic Speech Recognition (ASR) and Automatic Speech
  Translation (AST) training data for **25 European languages**.

  The first public release of **Granary** (≈ 643 k h ASR / ≈ 351 k h AST) was
  built from three openly available corpora:

  * `espnet/yodas2 <https://huggingface.co/datasets/espnet/yodas2>`_
  * `FBK-MT/mosel <https://huggingface.co/datasets/FBK-MT/mosel>`_
  * `PleIAs/YouTube-Commons <https://huggingface.co/datasets/PleIAs/YouTube-Commons>`_

  and is published as
  `nvidia/Granary <https://huggingface.co/datasets/nvidia/Granary>`_.

  .. note::

    **Per-language runs.** The pipeline is executed **once per language pair**:
    set

    * ``source_lang`` / ``source_lang_full`` – audio & transcript language
    * ``translation.target_lang`` / ``target_lang_full`` – translation language

    For example, to obtain English audio with Italian translations choose
    ``source_lang: en`` and ``translation.target_lang: it``.  
    Separate runs are required for each additional language combination.

  .. note::

    **GPU required.** All Whisper, vLLM and Comet-QE stages expect at
    least one CUDA-capable GPU.  Multi-GPU nodes are auto-detected when
    ``num_devices: -1`` (default) is used.

  Software prerequisites
  ----------------------

  Install *NeMo-speech-data-processor* **plus** the extra wheels required by
  specific processors:

  ``FasterWhisperInference``::

      pip install pytorch-lightning \
                  "nvidia-cublas-cu12" \
                  "nvidia-cudnn-cu12==9.*" \
                  faster_whisper

      export LD_LIBRARY_PATH=$(python - <<'PY'
      import os, nvidia.cublas.lib, nvidia.cudnn.lib
      print(os.path.dirname(nvidia.cublas.lib.__file__) + ":" +
            os.path.dirname(nvidia.cudnn.lib.__file__))
      PY)

  ``vLLMInference``::

      pip install "optree>=0.13.0" vllm

  ``CometoidWMTQualityEstimation``::

      pip install pymarian

  ``FastTextLangIdClassifier``::

      pip install fasttext

  ``ConvertToTarredAudioDataset`` *(optional, only if tar-sharding is enabled)*::

      pip install lhotse "nemo-toolkit[common]==2.2.1"

  Quick start
  -----------

  1.  **Hardware** – Linux box with NVIDIA GPU(s) and ≥ 16 GB VRAM (reference runs used A100-80 GB; smaller cards work with reduced batch sizes).

  2.  **Install** *NeMo-speech-data-processor* and the extras listed above.

  3.  **Prepare** the *input manifest* and set three mandatory YAML keys:

      * ``input_manifest_file`` – manifest with raw audio paths  
      * ``output_dir`` – working/output directory  
      * ``sdp_dir`` – root of the SDP tree (for prompt/regex assets)

  4.  **Run the pipeline**:

      .. code-block:: bash

          # Path to your local clone of NeMo-speech-data-processor
          SDP_DIR=/path/to/NeMo-speech-data-processor

          python ${SDP_DIR}/main.py \
              --config-path ${SDP_DIR}/dataset_configs/multilingual/granary/ \
              --config-name  config.yaml \
              input_manifest_file=/path/to/input_manifest.json \
              output_dir=/path/to/output/dir \
              sdp_dir=${SDP_DIR}

  Input and output formats
  ------------------------

  **Input manifest**

  Each line is a JSON object with the source-audio path::

    {"source_audio_filepath": "/path/to/file.flac"}

  **Key outputs**

  * ``${output_dir}/${source_lang}/manifest_46.json`` – final bilingual manifest
    containing ``audio_filepath``, ``offset``, ``duration``, ``text`` (source) and
    ``answer`` (translation), plus constant decoder flags.

  * ``${output_dir}/${source_lang}/tarred_dataset/`` – *(optional)* tarred-audio
    shards and ``shard_manifest.json`` when
    ``convert_to_audio_tarred_dataset.should_run: True``.

  * All intermediate ``manifest_XX.json`` files are kept for audit/debug.

  Pipeline stages
  ---------------

  The processors executed (indices match the config):

  #. **FfmpegConvert (0)** – re-encode audio to 16 kHz/mono FLAC.

  #. **GetAudioDuration (1)** – compute clip length.

  #. **RemoveFiles (2)** – optionally delete originals (``params.save_disk_space``).

  #. **FasterWhisperInference (3)** – *pass 1* language detection.

  #. **LambdaExpression (4)** – probability-based LID filtering.

  #. **DropSpecifiedFields (5)** – remove temporary fields.

  #. **FasterWhisperInference (6, 14)** – two-pass transcription (second run can slice by offset).

  #. **Segmentation & grooming (7–13)** – split Whisper segments into atomic utterances.

  #. **Hallucination detection (18–20)** – drop repeated n-grams, garbage tokens and common filler phrases.

  #. **PnC restoration (21–23)** – Qwen-2.5-7B restores punctuation & capitalisation; optional regex clean-up.

  #. **Length & charset filtering (27–36)** – word-ratio, character histogram and FastText checks.

  #. **Quality estimation (41–43)** – keep pairs with Comet-QE ``score ≥ min_qe_score``.

  #. **Constant flags (44)** – add decoder directives (``<|emo:undefined|>``, *itn*, *pnc*, etc.).

  #. **Tarred dataset (46)** – shard audio into ``num_shards`` tar files (optional).

  Tunable parameters
  ------------------

  All knobs live under the ``params`` block.

  * **Language**

    * ``source_lang`` / ``source_lang_full``
    * ``translation.target_lang`` / ``target_lang_full``

  * **Audio duration**

    * ``min_audio_duration`` – drop very short clips (seconds)
    * ``max_audio_duration`` – drop very long clips (seconds)

  * **Language-ID & text filtering**

    * ``min_audio_lid_probability`` – Whisper LID threshold
    * ``translation.min_hist_token_ratio`` – charset-purity ratio
    * ``translation.min_text_lid_probability`` – FastText LID threshold

  * **Length & quality**

    * ``translation.max_len_diff_ratio`` – max(src / tgt) word ratio
    * ``translation.min_qe_score`` – Comet-QE acceptance score

  * **Tarred dataset**

    * ``convert_to_audio_tarred_dataset.should_run`` (bool)
    * ``num_shards`` and ``buckets_num`` – shard layout

  * **Misc.**

    * ``use_regex`` – regex preset for text normalisation
    * ``save_disk_space`` – delete originals after conversion
    * ``use_dask`` – enable distributed execution (not recommended)

  Advanced usage
  --------------

  * **Selective execution** – override ``processors_to_run`` with a range of 
    indices, e.g. ``"0:25"``.

  * **Model swapping** – every inference processor exposes either
    ``model_size_or_path`` (Whisper) or an embedded ``model:`` block (vLLM).

  * **Resource tuning** – ``num_devices = -1`` uses all visible GPUs; set an
    integer to pin workers per stage.

  References
  ----------

  * Koluguri *et al.* (2025). *Granary: Speech Recognition and Translation
    Dataset in 25 European Languages* (preprint). arXiv:
    `2505.13404 <https://arxiv.org/abs/2505.13404>`_.

  * Granary dataset on Hugging Face:
    `nvidia/Granary <https://huggingface.co/datasets/nvidia/Granary>`_.

  * NeMo-SDP source code:
    `<https://github.com/NVIDIA/NeMo-speech-data-processor/blob/main/dataset_configs/multilingual/granary/>`_.

input_manifest_file: ?? 
output_dir: ??
sdp_dir: ?? #/path/to/NeMo-speech-data-processor
cache_dir: ${output_dir}/cache

params:
  source_lang: id
  source_lang_full: Indonesian
  min_audio_lid_probability:  0.7
  segment_duration: 10.0
  min_audio_duration: 0.1
  max_audio_duration: 30.0
  use_regex: common
  translation:
    target_lang: it
    target_lang_full: Italian
    max_len_diff_ratio: 4
    min_hist_token_ratio: 0.8
    min_text_lid_probability: 0.3
    min_qe_score: 0.75
  convert_to_audio_tarred_dataset:
    should_run: False
    num_shards: 16
    buckets_num: 1
  save_disk_space: False

processors_to_run: "all"
use_dask: False

processors:
  - _target_: sdp.processors.FfmpegConvert
    input_manifest_file: ${input_manifest_file}
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_00.json
    input_file_key: 'source_audio_filepath'
    output_file_key: 'audio_filepath'
    converted_audio_dir: ${output_dir}/${params.source_lang}/converted_audio/
    target_samplerate: 16000
    target_nchannels: 1
  
  - _target_: sdp.processors.GetAudioDuration
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_01.json
    audio_filepath_key: 'audio_filepath'
    duration_key: 'duration'
  
  - _target_: sdp.processors.RemoveFiles
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_02.json
    filepath_field: 'source_audio_filepath' 
    should_run: ${params.save_disk_space}

  - _target_: sdp.processors.FasterWhisperInference
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_03.json
    model_size_or_path: 'large-v3'
    num_devices: -1
    output_dir: ${output_dir}/${params.source_lang}/step_03
    language_detection_only: True
    inference:
        language_detection_segments: 7
        chunk_length: 30
    save_timestamps_separately: False
    skip_corrupted_audios: True

  - _target_: sdp.processors.LambdaExpression
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_04.json
    new_field: 'lid_verified'
    expression: (entry.language == "${params.source_lang}") & (entry.language_probability >= ${params.min_audio_lid_probability})
    filter: True

  - _target_: sdp.processors.DropSpecifiedFields
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_05.json
    fields_to_drop:
      - source_audio_filepath
      - language
      - language_probability
      - lid_verified
  
  - _target_: sdp.processors.FasterWhisperInference
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_06.json
    model_size_or_path: 'large-v3'
    output_dir: ${output_dir}/${params.source_lang}/step_06
    num_devices: -1
    inference:
        language: ${params.source_lang}
    save_timestamps_separately: False
    skip_corrupted_audios: True
  
  - _target_: sdp.processors.DropSpecifiedFields
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_07.json
    fields_to_drop:
      - duration
  
  - _target_: sdp.processors.ListToEntries
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_08.json
    field_with_list: 'segments'
  
  - _target_: sdp.processors.KeepOnlySpecifiedFields
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_09.json
    fields_to_keep:
      - audio_filepath
      - id
      - start
      - end
      - text
      - language
  
  - _target_: sdp.processors.LambdaExpression
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_10.json
    new_field: 'duration'
    expression: entry.end - entry.start
  
  - _target_: sdp.processors.DropHighLowDuration
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_11.json
    high_duration_threshold: ${params.max_audio_duration}
    low_duration_threshold: ${params.min_audio_duration}
  
  - _target_: sdp.processors.RenameFields
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_12.json
    rename_fields: 
      start: offset 
      id: segment_id 
      language: source_lang
  
  - _target_: sdp.processors.KeepOnlySpecifiedFields
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_13.json
    fields_to_keep:
      - source_lang
      - audio_filepath
      - segment_id
      - offset
      - duration

  - _target_: sdp.processors.SplitOnFixedDuration
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_13a.json
    segment_duration: ${params.segment_duration}
    drop_last: False
    drop_text: True

  - _target_: sdp.processors.FasterWhisperInference
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_14.json
    model_size_or_path: 'large-v3'
    num_devices: -1
    output_dir: ${output_dir}/${params.source_lang}/step_14
    inference:
        language: ${params.source_lang}
    save_timestamps_separately: False
    skip_corrupted_audios: True
    slice_by_offset: True
 
  - _target_: sdp.processors.KeepOnlySpecifiedFields
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_15.json
    fields_to_keep: 
      - source_lang
      - audio_filepath
      - segment_id
      - offset
      - duration
      - pred_text
  
  - _target_: sdp.processors.RenameFields
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_16.json
    rename_fields:
      pred_text: text

  - _target_: sdp.processors.DropIfRegexMatch
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_17.json
    text_key: text
    regex_patterns:
      - "^\\s*$"

  - _target_: sdp.processors.DetectWhisperHallucinationFeatures
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_18.json
    common_hall_file: ${sdp_dir}/dataset_configs/multilingual/granary/partials/common_phrases/${params.source_lang}.txt
    text_field: text
  
  - _target_: sdp.processors.LambdaExpression
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_19.json
    new_field:  is_hallucinated
    expression: (not entry.hall_repeated_ngrams) & (not entry.hall_long_word) & (not entry.hall_frequent_single_word)
    filter: True
    
  - _target_: sdp.processors.KeepOnlySpecifiedFields
    output_manifest_file: ${output_dir}/${params.source_lang}/manifest_20.json
    fields_to_keep: 
      - source_lang
      - audio_filepath
      - segment_id
      - offset
      - duration
      - text